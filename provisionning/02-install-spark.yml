---
# ansible-playbook -i inventories/jobjects 02-install-spark.yml
- name: Playbook to configure with username/password
  hosts: hadoop
  become: true

  vars_files:
    - vars/main.yml

  roles:
    - role: ansible_role_almalinux
    - role: geerlingguy.swap
    - role: ansible_role_hadoop_adduser

  tasks:

  ## Spark
    # curl -OL https://dlcdn.apache.org/hadoop/common/{{ install_hadoop_file }}/{{ install_hadoop_file_zip }}
    - name: Download file Spark3 for Hadoop3
      ansible.builtin.get_url:
        url: https://dlcdn.apache.org/spark/{{ install_spark_file }}/{{ install_spark_file_zip }}
        dest: "{{ install_hadoop_dir }}/{{ install_spark_file_zip }}"
        checksum: sha256:173651a8a00f5bf0ee27b74d817e0e52eed9daa49fd66d59718994974d1d367d
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: '0644'
        validate_certs: false
      when: inventory_hostname == hdfs_namenodes[0]
    - name: "Copy Remote-To-Remote from {{ hdfs_namenodes[0] }}"
      ansible.posix.synchronize:
        src: "{{ install_hadoop_dir }}/{{ install_spark_file_zip }}"
        dest: "{{ install_hadoop_dir }}/{{ install_spark_file_zip }}"
      delegate_to: "{{ hdfs_namenodes[0] }}"
    # sudo tar xvfz {{ install_hadoop_file_zip }} -C {{ install_hadoop_dir }}/
    - name: Unarchive a file that is already on the remote machine
      ansible.builtin.unarchive:
        src: "{{ install_hadoop_dir }}/{{ install_spark_file_zip }}"
        dest: "{{ install_hadoop_dir }}"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        creates: "{{ install_spark_dirname_unzip }}"
        remote_src: true
    - name: Add another bin dir to system-wide $PATH.
      ansible.builtin.copy:
        dest: /etc/profile.d/03-spark-path.sh
        mode: '0644'
        content: |
          export SPARK_HOME={{ install_spark_dirname_unzip }}
          export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
          export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
    - name: Copy Hive configuration in Spark conf directory
      ansible.builtin.template:
        src: "{{ item }}"
        dest: "{{ install_spark_dirname_unzip }}/conf"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: '0644'
      loop:
        - hive-site.xml
    - name: Install configure file in /etc/hadoop/{{ item }}
      ansible.builtin.template:
        src: "{{ item }}"
        dest: "{{ install_spark_dirname_unzip }}/conf/{{ item }}"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: '0644'
      loop:
        - spark-defaults.conf
    - name: Create command on HDFS CLuster for spark
      ansible.builtin.command: "{{ item }}"
      args:
        chdir: "{{ install_hadoop_dirname_unzip }}/bin"
      become: true
      become_user: "{{ hadoop_user }}"
      when: inventory_hostname == hdfs_namenodes[0]
      register: my_output
      changed_when: my_output.rc == 0
      loop:
        - bash -lc './hdfs dfs -mkdir -p /user/hadoop'
        - bash -lc './hdfs dfs -chown hadoop:hadoop /user/hadoop'
        - bash -lc './hdfs dfs -mkdir -p /tmp'
        - bash -lc './hdfs dfs -chmod g+w /tmp'
        - bash -lc './hdfs dfs -mkdir -p /tmp/spark-logs'
        - bash -lc './hdfs dfs -chmod 777 /tmp/spark-logs'
    - name: Copy postgresql jar on HDFS CLuster for spark
      ansible.builtin.command: "{{ item }}"
      args:
        chdir: "{{ install_hadoop_dirname_unzip }}/bin"
      become: true
      become_user: "{{ hadoop_user }}"
      register: my_output
      changed_when: my_output.rc == 0
      loop:
        - bash -lc 'curl -L -O --output-dir {{ install_spark_dirname_unzip }}/jars
          https://jdbc.postgresql.org/download/postgresql-42.7.4.jar'
    - name: "Copy services SPARK History Service"
      ansible.builtin.template:
        src: "systemd_service.template"
        dest: "/etc/systemd/system/{{ item.filename }}"
        owner: "root"
        group: "root"
        mode: '0644'
      when: inventory_hostname == hdfs_namenodes[0]
      loop:
        - { description: "SPARK History Service",
            start: "nohup /bin/bash -lc 'nohup {{ install_spark_dirname_unzip }}/sbin/start-history-server.sh >>/tmp/start-history-server.log 2>&1'",
            stop: "nohup /bin/bash -lc '{{ install_spark_dirname_unzip }}/sbin/stop-history-server.sh >>/tmp/stop-history-server.log 2>&1'",
            filename: "spark-history.service" }
    - name: Install SPARK History Service
      ansible.builtin.systemd_service:
        name: "{{ item }}"
        enabled: true
        state: started
        daemon_reload: true
      become: true
      when: inventory_hostname == hdfs_namenodes[0]
      with_items:
        - spark-history
